"""
FinBERT-based News Sentiment Analysis for Indian Banking Stocks

Improvements over VADER:
1. Financial domain-specific sentiment (earnings, quarterly results, etc.)
2. Better understanding of financial jargon and context
3. Multi-source news aggregation (BSE, NSE, MoneyControl, Economic Times)
4. Event detection (earnings, policy changes, regulatory news)
5. Weighted sentiment based on news source credibility

Uses: yiyanghkust/finbert-tone (fine-tuned for financial sentiment)
"""

import pandas as pd
import numpy as np
from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline
import torch
from datetime import datetime, timedelta
import os
import requests
from bs4 import BeautifulSoup
import json
import time
import warnings
warnings.filterwarnings('ignore')

print("=" * 80)
print("FinBERT NEWS SENTIMENT ANALYSIS".center(80))
print("Financial Domain-Specific Sentiment for Indian Banks".center(80))
print("=" * 80)

# =============================================================================
# CONFIGURATION
# =============================================================================

# FinBERT model from HuggingFace
FINBERT_MODEL = "yiyanghkust/finbert-tone"  # Pretrained on financial news

# Indian banking stocks
BANKING_STOCKS = {
    'Private Banks': {
        'HDFC Bank': {'ticker': 'HDFCBANK', 'bse_code': '500180', 'nse': 'HDFCBANK'},
        'ICICI Bank': {'ticker': 'ICICIBANK', 'bse_code': '532174', 'nse': 'ICICIBANK'},
        'Kotak Mahindra Bank': {'ticker': 'KOTAKBANK', 'bse_code': '500247', 'nse': 'KOTAKBANK'},
        'Axis Bank': {'ticker': 'AXISBANK', 'bse_code': '532215', 'nse': 'AXISBANK'}
    },
    'PSU Banks': {
        'State Bank of India': {'ticker': 'SBIN', 'bse_code': '500112', 'nse': 'SBIN'},
        'Punjab National Bank': {'ticker': 'PNB', 'bse_code': '532461', 'nse': 'PNB'},
        'Bank of Baroda': {'ticker': 'BANKBARODA', 'bse_code': '532134', 'nse': 'BANKBARODA'},
        'Canara Bank': {'ticker': 'CANBK', 'bse_code': '532483', 'nse': 'CANBK'}
    }
}

# News source weights (credibility scoring)
NEWS_SOURCE_WEIGHTS = {
    'bseindia.com': 1.0,      # Official BSE
    'nseindia.com': 1.0,       # Official NSE
    'moneycontrol.com': 0.9,   # High credibility
    'economictimes.com': 0.9,  # High credibility
    'livemint.com': 0.85,      # Good credibility
    'business-standard.com': 0.85,
    'reuters.com': 0.9,
    'bloomberg.com': 0.9,
    'default': 0.5             # Unknown sources
}

# Event keywords for financial news
EVENT_KEYWORDS = {
    'earnings': ['quarterly results', 'earnings', 'Q1', 'Q2', 'Q3', 'Q4', 'profit', 'revenue', 'EPS'],
    'policy': ['RBI', 'monetary policy', 'repo rate', 'CRR', 'SLR', 'regulation'],
    'expansion': ['branch', 'merger', 'acquisition', 'expansion', 'new product'],
    'risk': ['NPA', 'bad loans', 'default', 'fraud', 'investigation', 'penalty'],
    'leadership': ['CEO', 'MD', 'board', 'appointment', 'resignation']
}

# Create directories
os.makedirs('data/finbert_sentiment', exist_ok=True)
os.makedirs('data/finbert_sentiment/private_banks', exist_ok=True)
os.makedirs('data/finbert_sentiment/psu_banks', exist_ok=True)

# =============================================================================
# LOAD FINBERT MODEL
# =============================================================================

print("\nðŸ“¦ Loading FinBERT model...")
print(f"   Model: {FINBERT_MODEL}")

device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"   Device: {device.upper()}")

# Load FinBERT sentiment analysis pipeline
finbert = pipeline(
    "sentiment-analysis",
    model=FINBERT_MODEL,
    tokenizer=FINBERT_MODEL,
    device=0 if device == "cuda" else -1,
    max_length=512,
    truncation=True
)

print("   âœ“ FinBERT loaded successfully!\n")

# =============================================================================
# NEWS SCRAPING FUNCTIONS
# =============================================================================

def scrape_moneycontrol_news(stock_name, ticker, days=7):
    """Scrape MoneyControl for recent news"""
    news_items = []

    try:
        # MoneyControl stock page (simplified - would need actual URL structure)
        url = f"https://www.moneycontrol.com/news/business/stocks/{ticker.lower()}"
        headers = {'User-Agent': 'Mozilla/5.0'}

        # Note: This is a placeholder - actual implementation would need proper scraping
        print(f"      â„¹ MoneyControl scraping placeholder for {stock_name}")

    except Exception as e:
        print(f"      âš  MoneyControl error: {e}")

    return news_items

def scrape_economictimes_news(stock_name, ticker, days=7):
    """Scrape Economic Times for recent news"""
    news_items = []

    try:
        # Economic Times search (simplified)
        search_query = stock_name.replace(' ', '+')
        url = f"https://economictimes.indiatimes.com/topic/{search_query}"
        headers = {'User-Agent': 'Mozilla/5.0'}

        print(f"      â„¹ Economic Times scraping placeholder for {stock_name}")

    except Exception as e:
        print(f"      âš  Economic Times error: {e}")

    return news_items

def get_gnews_articles(stock_name, ticker, days=7):
    """Get news from GNews API (existing integration)"""
    from gnews import GNews

    news_items = []

    try:
        google_news = GNews(language='en', country='IN', period=f'{days}d', max_results=50)
        articles = google_news.get_news(stock_name)

        for article in articles:
            news_items.append({
                'date': article.get('published date', datetime.now().strftime('%Y-%m-%d')),
                'title': article.get('title', ''),
                'description': article.get('description', ''),
                'source': article.get('publisher', {}).get('title', 'GNews'),
                'url': article.get('url', '')
            })

    except Exception as e:
        print(f"      âš  GNews error: {e}")

    return news_items

# =============================================================================
# FINBERT SENTIMENT ANALYSIS
# =============================================================================

def analyze_with_finbert(text):
    """
    Analyze text using FinBERT

    Returns: {
        'label': 'positive'|'negative'|'neutral',
        'score': float (0-1 confidence)
    }
    """
    if not text or len(text.strip()) < 10:
        return {'label': 'neutral', 'score': 0.0}

    try:
        # Truncate if too long
        text = text[:512]
        result = finbert(text)[0]

        return {
            'label': result['label'].lower(),
            'score': round(result['score'], 4)
        }
    except Exception as e:
        print(f"      âš  FinBERT error: {e}")
        return {'label': 'neutral', 'score': 0.0}

def detect_event_type(text):
    """Detect financial event type from text"""
    text_lower = text.lower()
    detected_events = []

    for event_type, keywords in EVENT_KEYWORDS.items():
        if any(keyword.lower() in text_lower for keyword in keywords):
            detected_events.append(event_type)

    return detected_events if detected_events else ['general']

def get_source_weight(url):
    """Get credibility weight for news source"""
    for domain, weight in NEWS_SOURCE_WEIGHTS.items():
        if domain in url.lower():
            return weight
    return NEWS_SOURCE_WEIGHTS['default']

# =============================================================================
# PROCESS NEWS FOR ALL STOCKS
# =============================================================================

def process_stock_news(stock_name, stock_info, sector):
    """Process news for a single stock with FinBERT sentiment"""

    ticker = stock_info['ticker']
    print(f"\n{'='*60}")
    print(f"ðŸ“° {stock_name} ({ticker})")
    print(f"{'='*60}")

    # Collect news from multiple sources
    all_news = []

    print("   ðŸ“¡ Collecting news from sources...")

    # GNews (fallback/supplementary)
    gnews_items = get_gnews_articles(stock_name, ticker, days=7)
    print(f"      â€¢ GNews: {len(gnews_items)} articles")
    all_news.extend(gnews_items)

    # MoneyControl (would need proper scraping)
    # mc_items = scrape_moneycontrol_news(stock_name, ticker)
    # all_news.extend(mc_items)

    # Economic Times (would need proper scraping)
    # et_items = scrape_economictimes_news(stock_name, ticker)
    # all_news.extend(et_items)

    if not all_news:
        print("      âš  No news found")
        return None

    print(f"\n   ðŸ¤– Analyzing {len(all_news)} articles with FinBERT...")

    # Analyze each article with FinBERT
    analyzed_news = []

    for idx, article in enumerate(all_news, 1):
        # Combine title + description for better context
        full_text = f"{article.get('title', '')} {article.get('description', '')}"

        # FinBERT sentiment
        sentiment = analyze_with_finbert(full_text)

        # Event detection
        events = detect_event_type(full_text)

        # Source credibility
        source_weight = get_source_weight(article.get('url', ''))

        # Weighted sentiment score
        weighted_score = sentiment['score'] * source_weight

        analyzed_news.append({
            'date': article.get('date'),
            'title': article.get('title'),
            'description': article.get('description'),
            'source': article.get('source'),
            'url': article.get('url'),
            'sentiment_label': sentiment['label'],
            'sentiment_score': sentiment['score'],
            'source_weight': source_weight,
            'weighted_sentiment': weighted_score,
            'event_types': ','.join(events),
            'is_earnings': 'earnings' in events,
            'is_policy': 'policy' in events,
            'is_risk': 'risk' in events
        })

        if idx % 10 == 0:
            print(f"      âœ“ Analyzed {idx}/{len(all_news)} articles...")

    # Create DataFrame
    df = pd.DataFrame(analyzed_news)
    df['date'] = pd.to_datetime(df['date'])
    df = df.sort_values('date', ascending=False)

    # Calculate daily aggregated sentiment
    daily_sentiment = df.groupby('date').agg({
        'weighted_sentiment': 'mean',
        'sentiment_score': 'mean',
        'title': 'count',
        'is_earnings': 'max',
        'is_policy': 'max',
        'is_risk': 'max'
    }).rename(columns={'title': 'news_count'})

    # Sentiment polarity (-1 to +1)
    daily_sentiment['sentiment_polarity'] = daily_sentiment.apply(
        lambda row: row['weighted_sentiment'] if df[df['date'] == row.name]['sentiment_label'].mode()[0] == 'positive'
        else -row['weighted_sentiment'] if df[df['date'] == row.name]['sentiment_label'].mode()[0] == 'negative'
        else 0, axis=1
    )

    # Save detailed news
    sector_dir = 'private_banks' if sector == 'Private Banks' else 'psu_banks'
    detailed_file = f"data/finbert_sentiment/{sector_dir}/{ticker}_news_detailed.csv"
    df.to_csv(detailed_file, index=False)

    # Save daily aggregated sentiment
    daily_file = f"data/finbert_sentiment/{sector_dir}/{ticker}_daily_sentiment.csv"
    daily_sentiment.to_csv(daily_file)

    print(f"\n   âœ“ Saved:")
    print(f"      â€¢ Detailed: {detailed_file}")
    print(f"      â€¢ Daily: {daily_file}")

    # Summary
    print(f"\n   ðŸ“Š Summary:")
    print(f"      â€¢ Total articles: {len(df)}")
    print(f"      â€¢ Positive: {len(df[df['sentiment_label']=='positive'])}")
    print(f"      â€¢ Negative: {len(df[df['sentiment_label']=='negative'])}")
    print(f"      â€¢ Neutral: {len(df[df['sentiment_label']=='neutral'])}")
    print(f"      â€¢ Avg sentiment: {df['weighted_sentiment'].mean():.3f}")
    print(f"      â€¢ Earnings news: {df['is_earnings'].sum()} articles")

    return df

# =============================================================================
# MAIN EXECUTION
# =============================================================================

if __name__ == "__main__":

    all_results = []

    for sector, banks in BANKING_STOCKS.items():
        print(f"\n\n{'#'*80}")
        print(f"# {sector.upper()}")
        print(f"{'#'*80}")

        for stock_name, stock_info in banks.items():
            result = process_stock_news(stock_name, stock_info, sector)
            if result is not None:
                all_results.append({
                    'stock': stock_name,
                    'ticker': stock_info['ticker'],
                    'articles': len(result),
                    'avg_sentiment': result['weighted_sentiment'].mean()
                })

            # Rate limiting
            time.sleep(2)

    # Save summary
    summary_df = pd.DataFrame(all_results)
    summary_df.to_csv('data/finbert_sentiment/sentiment_summary.csv', index=False)

    print("\n\n" + "="*80)
    print("âœ“ FinBERT SENTIMENT ANALYSIS COMPLETE".center(80))
    print("="*80)
    print(f"\nProcessed {len(all_results)} stocks")
    print(f"Summary saved to: data/finbert_sentiment/sentiment_summary.csv")
